{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "sc\n",
    "import ujson\n",
    "import pickle\n",
    "\n",
    "with open(\"../Data/hw2-files.txt\") as f:\n",
    "    files = [l.strip() for l in f.readlines()]\n",
    "RDD = sc.textFile(','.join(files))\n",
    "#RDD = RDD.map(lambda text: text.encode('utf-8')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements: 2193\n"
     ]
    }
   ],
   "source": [
    "def print_count(rdd):\n",
    "    print 'Number of elements:', rdd.count()\n",
    "    \n",
    "print_count(RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique users is: 2083\n",
      "CPU times: user 8 ms, sys: 4 ms, total: 12 ms\n",
      "Wall time: 242 ms\n"
     ]
    }
   ],
   "source": [
    " %%time\n",
    "\n",
    "# TRying to convert raw data to json :\n",
    "\n",
    "def safe_parse(myjson):\n",
    "    '''\n",
    "    safe parsing json string\n",
    "    '''\n",
    "    try:\n",
    "        json_object = ujson.loads(myjson)\n",
    "    except ValueError, e:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "valid_RDD = RDD.filter(lambda text: safe_parse(text))\n",
    "RDD_json = valid_RDD.map(lambda text: ujson.loads(text))\n",
    "# Filtering out all limit messaged tweets\n",
    "valid_RDD = RDD_json.filter(lambda tweet: 'limit' not in tweet)\n",
    "\n",
    "paired_RDD = valid_RDD.map(lambda tweet: (tweet['user']['id_str'], tweet['text'].encode('utf-8'))) \n",
    "\n",
    "UserTweetRDD=paired_RDD.groupByKey().mapValues(lambda x:[a for a in x]).cache()\n",
    "\n",
    "def print_users_count(count):\n",
    "    print 'The number of unique users is:', count\n",
    "    \n",
    "print_users_count(UserTweetRDD.count())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0 posted 81 tweets\n",
      "Group 1 posted 199 tweets\n",
      "Group 2 posted 45 tweets\n",
      "Group 3 posted 313 tweets\n",
      "Group 4 posted 86 tweets\n",
      "Group 5 posted 221 tweets\n",
      "Group 6 posted 400 tweets\n",
      "Group 7 posted 798 tweets\n",
      "CPU times: user 5.64 s, sys: 32 ms, total: 5.68 s\n",
      "Wall time: 6.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "PickleFile=pickle.load( open(\"../Data/users-partition.pickle\"))\n",
    "\n",
    "def findMatch(user):\n",
    "    if user[0] in PickleFile:\n",
    "        return ((PickleFile[user[0]]),1)\n",
    "    else:\n",
    "        return (7,1)\n",
    "    \n",
    "UserMatchRDD= paired_RDD.map(lambda user:findMatch(user))\\\n",
    "                .reduceByKey(lambda x,y:x+y)\\\n",
    "                             .sortByKey()\n",
    "    \n",
    " \n",
    "def print_post_count(counts):\n",
    "    for group_id, count in counts:\n",
    "        print 'Group %d posted %d tweets' % (group_id, count)\n",
    "\n",
    "PartitionCount=UserMatchRDD.collect()\n",
    "\n",
    "print_post_count(PartitionCount)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PART 3\n",
    "\n",
    "# %load happyfuntokenizing.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "This code implements a basic, Twitter-aware tokenizer.\n",
    "\n",
    "A tokenizer is a function that splits a string of text into words. In\n",
    "Python terms, we map string and unicode objects into lists of unicode\n",
    "objects.\n",
    "\n",
    "There is not a single right way to do tokenizing. The best method\n",
    "depends on the application.  This tokenizer is designed to be flexible\n",
    "and this easy to adapt to new domains and tasks.  The basic logic is\n",
    "this:\n",
    "\n",
    "1. The tuple regex_strings defines a list of regular expression\n",
    "   strings.\n",
    "\n",
    "2. The regex_strings strings are put, in order, into a compiled\n",
    "   regular expression object called word_re.\n",
    "\n",
    "3. The tokenization is done by word_re.findall(s), where s is the\n",
    "   user-supplied string, inside the tokenize() method of the class\n",
    "   Tokenizer.\n",
    "\n",
    "4. When instantiating Tokenizer objects, there is a single option:\n",
    "   preserve_case.  By default, it is set to True. If it is set to\n",
    "   False, then the tokenizer will downcase everything except for\n",
    "   emoticons.\n",
    "\n",
    "The __main__ method illustrates by tokenizing a few examples.\n",
    "\n",
    "I've also included a Tokenizer method tokenize_random_tweet(). If the\n",
    "twitter library is installed (http://code.google.com/p/python-twitter/)\n",
    "and Twitter is cooperating, then it should tokenize a random\n",
    "English-language tweet.\n",
    "\n",
    "\n",
    "Julaiti Alafate:\n",
    "  I modified the regex strings to extract URLs in tweets.\n",
    "\"\"\"\n",
    "\n",
    "__author__ = \"Christopher Potts\"\n",
    "__copyright__ = \"Copyright 2011, Christopher Potts\"\n",
    "__credits__ = []\n",
    "__license__ = \"Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License: http://creativecommons.org/licenses/by-nc-sa/3.0/\"\n",
    "__version__ = \"1.0\"\n",
    "__maintainer__ = \"Christopher Potts\"\n",
    "__email__ = \"See the author's website\"\n",
    "\n",
    "######################################################################\n",
    "\n",
    "import re\n",
    "import htmlentitydefs\n",
    "\n",
    "######################################################################\n",
    "# The following strings are components in the regular expression\n",
    "# that is used for tokenizing. It's important that phone_number\n",
    "# appears first in the final regex (since it can contain whitespace).\n",
    "# It also could matter that tags comes after emoticons, due to the\n",
    "# possibility of having text like\n",
    "#\n",
    "#     <:| and some text >:)\n",
    "#\n",
    "# Most imporatantly, the final element should always be last, since it\n",
    "# does a last ditch whitespace-based tokenization of whatever is left.\n",
    "\n",
    "# This particular element is used in a couple ways, so we define it\n",
    "# with a name:\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "\n",
    "# The components of the tokenizer:\n",
    "regex_strings = (\n",
    "    # Phone numbers:\n",
    "    r\"\"\"\n",
    "    (?:\n",
    "      (?:            # (international)\n",
    "        \\+?[01]\n",
    "        [\\-\\s.]*\n",
    "      )?            \n",
    "      (?:            # (area code)\n",
    "        [\\(]?\n",
    "        \\d{3}\n",
    "        [\\-\\s.\\)]*\n",
    "      )?    \n",
    "      \\d{3}          # exchange\n",
    "      [\\-\\s.]*   \n",
    "      \\d{4}          # base\n",
    "    )\"\"\"\n",
    "    ,\n",
    "    # URLs:\n",
    "    r\"\"\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\"\"\"\n",
    "    ,\n",
    "    # Emoticons:\n",
    "    emoticon_string\n",
    "    ,    \n",
    "    # HTML tags:\n",
    "     r\"\"\"<[^>]+>\"\"\"\n",
    "    ,\n",
    "    # Twitter username:\n",
    "    r\"\"\"(?:@[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Twitter hashtags:\n",
    "    r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Remaining word types:\n",
    "    r\"\"\"\n",
    "    (?:[a-z][a-z'\\-_]+[a-z])       # Words with apostrophes or dashes.\n",
    "    |\n",
    "    (?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)  # Numbers, including fractions, decimals.\n",
    "    |\n",
    "    (?:[\\w_]+)                     # Words without apostrophes or dashes.\n",
    "    |\n",
    "    (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots. \n",
    "    |\n",
    "    (?:\\S)                         # Everything else that isn't whitespace.\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "######################################################################\n",
    "# This is the core tokenizing regex:\n",
    "    \n",
    "word_re = re.compile(r\"\"\"(%s)\"\"\" % \"|\".join(regex_strings), re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "# The emoticon string gets its own regex so that we can preserve case for them as needed:\n",
    "emoticon_re = re.compile(regex_strings[1], re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "# These are for regularizing HTML entities to Unicode:\n",
    "html_entity_digit_re = re.compile(r\"&#\\d+;\")\n",
    "html_entity_alpha_re = re.compile(r\"&\\w+;\")\n",
    "amp = \"&amp;\"\n",
    "\n",
    "######################################################################\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, preserve_case=False):\n",
    "        self.preserve_case = preserve_case\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        \"\"\"\n",
    "        Argument: s -- any string or unicode object\n",
    "        Value: a tokenize list of strings; conatenating this list returns the original string if preserve_case=False\n",
    "        \"\"\"        \n",
    "        # Try to ensure unicode:\n",
    "        try:\n",
    "            s = unicode(s)\n",
    "        except UnicodeDecodeError:\n",
    "            s = str(s).encode('string_escape')\n",
    "            s = unicode(s)\n",
    "        # Fix HTML character entitites:\n",
    "        s = self.__html2unicode(s)\n",
    "        # Tokenize:\n",
    "        words = word_re.findall(s)\n",
    "        # Possible alter the case, but avoid changing emoticons like :D into :d:\n",
    "        if not self.preserve_case:            \n",
    "            words = map((lambda x : x if emoticon_re.search(x) else x.lower()), words)\n",
    "        return words\n",
    "\n",
    "    def tokenize_random_tweet(self):\n",
    "        \"\"\"\n",
    "        If the twitter library is installed and a twitter connection\n",
    "        can be established, then tokenize a random tweet.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import twitter\n",
    "        except ImportError:\n",
    "            print \"Apologies. The random tweet functionality requires the Python twitter library: http://code.google.com/p/python-twitter/\"\n",
    "        from random import shuffle\n",
    "        api = twitter.Api()\n",
    "        tweets = api.GetPublicTimeline()\n",
    "        if tweets:\n",
    "            for tweet in tweets:\n",
    "                if tweet.user.lang == 'en':            \n",
    "                    return self.tokenize(tweet.text)\n",
    "        else:\n",
    "            raise Exception(\"Apologies. I couldn't get Twitter to give me a public English-language tweet. Perhaps try again\")\n",
    "\n",
    "    def __html2unicode(self, s):\n",
    "        \"\"\"\n",
    "        Internal metod that seeks to replace all the HTML entities in\n",
    "        s with their corresponding unicode characters.\n",
    "        \"\"\"\n",
    "        # First the digits:\n",
    "        ents = set(html_entity_digit_re.findall(s))\n",
    "        if len(ents) > 0:\n",
    "            for ent in ents:\n",
    "                entnum = ent[2:-1]\n",
    "                try:\n",
    "                    entnum = int(entnum)\n",
    "                    s = s.replace(ent, unichr(entnum))\t\n",
    "                except:\n",
    "                    pass\n",
    "        # Now the alpha versions:\n",
    "        ents = set(html_entity_alpha_re.findall(s))\n",
    "        ents = filter((lambda x : x != amp), ents)\n",
    "        for ent in ents:\n",
    "            entname = ent[1:-1]\n",
    "            try:            \n",
    "                s = s.replace(ent, unichr(htmlentitydefs.name2codepoint[entname]))\n",
    "            except:\n",
    "                pass                    \n",
    "            s = s.replace(amp, \" and \")\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements:  8979\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "tok = Tokenizer(preserve_case=False)\n",
    "\n",
    "tokenRDD=paired_RDD.flatMap(lambda line:tok.tokenize(line[1]))\n",
    "distinctTokens= tokenRDD.distinct()\n",
    "distinctTokens.count()\n",
    "\n",
    "def print_count(count):\n",
    "    print \"Number of elements: \",count\n",
    "print_count(distinctTokens.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements:  52\n",
      "===== overall =====\n",
      ":\t1386.0000\n",
      "rt\t1237.0000\n",
      ".\t865.0000\n",
      "\\\t745.0000\n",
      "the\t621.0000\n",
      "trump\t595.0000\n",
      "x80\t545.0000\n",
      "xe2\t543.0000\n",
      "to\t499.0000\n",
      ",\t489.0000\n",
      "xa6\t457.0000\n",
      "a\t403.0000\n",
      "is\t376.0000\n",
      "in\t296.0000\n",
      "'\t294.0000\n",
      "of\t292.0000\n",
      "and\t287.0000\n",
      "for\t280.0000\n",
      "!\t269.0000\n",
      "?\t210.0000\n",
      "\n",
      "CPU times: user 64 ms, sys: 8 ms, total: 72 ms\n",
      "Wall time: 706 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "UserTweet_Tokens=UserTweetRDD.flatMapValues(lambda x: x )\\\n",
    "                             .flatMapValues(lambda y: tok.tokenize(y) ) \n",
    "    \n",
    "DistinctUserTokens = UserTweet_Tokens.distinct()\n",
    "\n",
    "Token_UserList=DistinctUserTokens.map(lambda a: (a[1],a[0]))\\\n",
    "                         .groupByKey().mapValues(lambda x:[a for a in x]) \n",
    "    \n",
    "OverallTokensCount=Token_UserList.map(lambda x: (len(x[1]),x[0]))\\\n",
    "            .sortByKey(False)\n",
    " \n",
    "Token100=OverallTokensCount.filter(lambda a: a[0]>=100)\n",
    "C_all=Token100.map(lambda l:(l[1],l[0]))\n",
    "print_count(C_all.count())\n",
    "\n",
    "def get_rel_popularity(c_k, c_all):\n",
    "    return log(1.0 * c_k / c_all) / log(2)\n",
    "\n",
    "\n",
    "def print_tokens(tokens, gid = None):\n",
    "    group_name = \"overall\"\n",
    "    if gid is not None:\n",
    "        group_name = \"group %d\" % gid\n",
    "    print '=' * 5 + ' ' + group_name + ' ' + '=' * 5\n",
    "    for t, n in tokens:\n",
    "        print \"%s\\t%.4f\" % (t, n)\n",
    "    print\n",
    "    \n",
    "print_tokens(C_all.take(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== group 0 =====\n",
      "...\t-3.5648\n",
      "at\t-3.5983\n",
      "hillary\t-4.0484\n",
      "bernie\t-4.1430\n",
      "not\t-4.2479\n",
      "he\t-4.2574\n",
      "i\t-4.2854\n",
      "s\t-4.3309\n",
      "are\t-4.3646\n",
      "in\t-4.4021\n",
      "\n",
      "===== group 1 =====\n",
      "#demdebate\t-2.4391\n",
      "-\t-2.6202\n",
      "clinton\t-2.7174\n",
      "&\t-2.7472\n",
      "amp\t-2.7472\n",
      ";\t-2.7980\n",
      "sanders\t-2.8745\n",
      "?\t-2.9069\n",
      "in\t-2.9615\n",
      "if\t-2.9861\n",
      "\n",
      "===== group 2 =====\n",
      "are\t-4.6865\n",
      "and\t-4.7055\n",
      "bernie\t-4.7279\n",
      "at\t-4.7682\n",
      "sanders\t-4.9449\n",
      "in\t-5.0395\n",
      "donald\t-5.0531\n",
      "a\t-5.0697\n",
      "#demdebate\t-5.1396\n",
      "that\t-5.1599\n",
      "\n",
      "===== group 3 =====\n",
      "#demdebate\t-1.3847\n",
      "bernie\t-1.8535\n",
      "sanders\t-2.1793\n",
      "of\t-2.2356\n",
      "t\t-2.2675\n",
      "clinton\t-2.4179\n",
      "hillary\t-2.4203\n",
      "the\t-2.4330\n",
      "xa6\t-2.4962\n",
      "that\t-2.5160\n",
      "\n",
      "===== group 4 =====\n",
      "hillary\t-3.8074\n",
      "sanders\t-3.9449\n",
      "of\t-4.0199\n",
      "what\t-4.0875\n",
      "clinton\t-4.0959\n",
      "at\t-4.1832\n",
      "in\t-4.2095\n",
      "a\t-4.2623\n",
      "on\t-4.2854\n",
      "'\t-4.2928\n",
      "\n",
      "===== group 5 =====\n",
      "cruz\t-2.3344\n",
      "he\t-2.6724\n",
      "will\t-2.7705\n",
      "are\t-2.7796\n",
      "the\t-2.8522\n",
      "is\t-2.8822\n",
      "that\t-2.9119\n",
      "this\t-2.9542\n",
      "for\t-2.9594\n",
      "of\t-2.9804\n",
      "\n",
      "===== group 6 =====\n",
      "@realdonaldtrump\t-1.1520\n",
      "cruz\t-1.4657\n",
      "n\t-1.4877\n",
      "!\t-1.5479\n",
      "not\t-1.8904\n",
      "xa6\t-1.9172\n",
      "xe2\t-1.9973\n",
      "/\t-2.0238\n",
      "x80\t-2.0240\n",
      "it\t-2.0506\n",
      "\n",
      "===== group 7 =====\n",
      "donald\t-0.6471\n",
      "...\t-0.7922\n",
      "sanders\t-1.0380\n",
      "what\t-1.1178\n",
      "trump\t-1.1293\n",
      "bernie\t-1.2044\n",
      "you\t-1.2099\n",
      "-\t-1.2253\n",
      "if\t-1.2602\n",
      "clinton\t-1.2681\n",
      "\n",
      "CPU times: user 3.84 s, sys: 36 ms, total: 3.88 s\n",
      "Wall time: 5.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TopTokens=C_all.map(lambda l:l[0])\n",
    "TList=TopTokens.collect() \n",
    "Top100Result=Token_UserList.filter(lambda x: x[0] in TList)\\\n",
    "                            .flatMapValues(lambda x: x ) \n",
    "\n",
    "def findPartitionofUser(user):\n",
    "    if user[1] in PickleFile:\n",
    "        return (PickleFile[user[1]],user[0])\n",
    "    else:\n",
    "        return (7,user[0])\n",
    "\n",
    "Token_PartitionList= Top100Result.map(lambda a:findPartitionofUser(a)) \n",
    "\n",
    "Token_PartitionCount = Token_PartitionList.map(lambda a: (a,1))\\\n",
    "                                         .reduceByKey(lambda x,y:x+y) \n",
    "Partition_wiseTokenCount = Token_PartitionCount.map(lambda x: (x[0][0],(x[0][1],x[1])))\\\n",
    "                .groupByKey().mapValues(lambda x:[a for a in x])\\\n",
    "                .flatMapValues(lambda x: x)\\\n",
    "                .map(lambda x: (x[1][0],(x[0],x[1][1]))) \n",
    "            \n",
    "FinalRDD=Partition_wiseTokenCount.join(C_all)\\\n",
    "                        .map(lambda a: (a[1][0][0],(a[0],a[1][0][1],a[1][1])))\\\n",
    "                        .sortByKey().cache()\n",
    " \n",
    "RelPopularRDD=FinalRDD.map(lambda a: (get_rel_popularity(a[1][1], a[1][2]),(a[1][0],a[0])) )\n",
    "\n",
    "SortedRDD=RelPopularRDD.map(lambda a: ((-a[0],a[1][0]),(a[1][0],a[1][1])) )\\\n",
    "                             .sortByKey() \n",
    "RDDPrint=SortedRDD.map(lambda a: (a[0][1],-a[0][0],a[1][1])).cache()\n",
    "\n",
    "for i in range(0,8):\n",
    "    ResultRDD=RDDPrint.filter(lambda x: x[2]==i).map(lambda a:(a[0],a[1]))\n",
    "    print_tokens(ResultRDD.take(10),i)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users from group 3 are most likely to support Bernie Sanders.\n",
      "Users from group 5 are most likely to support Ted Cruz.\n",
      "Users from group 6 are most likely to support Donald Trump.\n"
     ]
    }
   ],
   "source": [
    "# Change the values of the following three items to your guesses\n",
    "\n",
    "users_support = [\n",
    "    (3, \"Bernie Sanders\"),\n",
    "    (5, \"Ted Cruz\"),\n",
    "    (6, \"Donald Trump\")\n",
    "]\n",
    "\n",
    "for gid, candidate in users_support:\n",
    "    print \"Users from group %d are most likely to support %s.\" % (gid, candidate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
